![Orpheus-FASTAPI Banner](https://lex-au.github.io/Orpheus-FastAPI/Banner.png)

# üöÄ ORPHEUS-FASTAPI 2026 EDITION

[![GitHub](https://img.shields.io/github/license/Lex-au/Orpheus-FastAPI)](https://github.com/Lex-au/Orpheus-FastAPI/blob/main/LICENSE.txt)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-00a393.svg)](https://fastapi.tiangolo.com/)

> **Next-Generation Neural Text-to-Speech Engine** with OpenAI-compatible API, 8 distinct AI voices, emotion tags, and a stunning futuristic web interface. Optimized for RTX GPUs with out-of-the-box LM Studio API support.

[GitHub Repository](https://github.com/TheLocalLab/Orpheus-FastAPI-LMStudio)

---

## ‚ú® What's New in 2026 Edition

- üé® **Futuristic Cyberpunk UI** - Glassmorphism design with neon accents
- ‚ö° **Async-First Architecture** - Non-blocking speech generation
- üîå **Dual API Endpoints** - Both `/v1/audio/speech` and `/v1/speech/audio`
- üìä **Enhanced Monitoring** - Health checks, request IDs, and timing headers
- üõ°Ô∏è **Better Error Handling** - OpenAI-compatible error responses
- üéØ **Input Validation** - Pydantic v2 models with strict validation

---

## üéôÔ∏è Voice Demos

Listen to sample outputs with different voices and emotions:
- [Default Test Sample](https://lex-au.github.io/Orpheus-FastAPI/DefaultTest.mp3) - Standard neutral tone
- [Leah Happy Sample](https://lex-au.github.io/Orpheus-FastAPI/LeahHappy.mp3) - Cheerful, upbeat demo
- [Tara Sad Sample](https://lex-au.github.io/Orpheus-FastAPI/TaraSad.mp3) - Emotional, melancholic demo
- [Zac Contemplative Sample](https://lex-au.github.io/Orpheus-FastAPI/ZacContemplative.mp3) - Thoughtful, measured tone

---

## üñ•Ô∏è User Interface

![Web User Interface](https://lex-au.github.io/Orpheus-FastAPI/WebUI.png)

---

## üåü Features

| Feature | Description |
|---------|-------------|
| üîó **OpenAI API Compatible** | Drop-in replacement for `/v1/audio/speech` endpoint |
| üé≠ **8 Neural Voices** | Diverse voice profiles with unique characteristics |
| üí¨ **Emotion Tags** | Add laughter, sighs, gasps, and more |
| ‚ö° **Real-time Generation** | Optimized for RTX GPUs with CUDA acceleration |
| üåê **Modern Web UI** | Responsive interface with waveform visualization |
| üì° **LM Studio Ready** | Works out-of-the-box with LM Studio Server API |

---

## üìÅ Project Structure

```
Orpheus-FastAPI/
‚îú‚îÄ‚îÄ app.py                # FastAPI server with OpenAI-compatible endpoints
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies
‚îú‚îÄ‚îÄ static/               # Static assets (favicon, images)
‚îú‚îÄ‚îÄ outputs/              # Generated audio files
‚îú‚îÄ‚îÄ templates/            # Jinja2 HTML templates
‚îÇ   ‚îî‚îÄ‚îÄ tts.html          # Futuristic web UI
‚îî‚îÄ‚îÄ tts_engine/           # Core TTS functionality
    ‚îú‚îÄ‚îÄ __init__.py       # Package exports
    ‚îú‚îÄ‚îÄ inference.py      # Token generation & API handling
    ‚îî‚îÄ‚îÄ speechpipe.py     # SNAC audio conversion pipeline
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- **For GPU acceleration (recommended)**:
  - NVIDIA GPU with CUDA support (RTX series recommended), or
  - Apple Silicon (M1/M2/M3 with MPS support)
- **CPU-only mode** is also supported but will be slower
- LM Studio or compatible LLM inference server

### Installation

```bash
# Clone the repository
git clone https://github.com/TheLocalLab/Orpheus-FastAPI-LMStudio.git
cd Orpheus-FastAPI-LMStudio

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install PyTorch (choose based on your system)

# For NVIDIA GPU (CUDA 12.4):
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# For NVIDIA GPU (CUDA 11.8):
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For Apple Silicon (M1/M2/M3):
pip3 install torch torchvision torchaudio

# For CPU only:
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Install other dependencies
pip3 install -r requirements.txt

# Create required directories
mkdir -p outputs static
```

### Verify Installation

After installation, run the validation script to check your setup:

```bash
python validate_setup.py
```

This will check:
- Python version compatibility
- All required dependencies
- PyTorch and device detection (CPU/GPU)
- Directory structure
- TTS engine configuration

### Start the Server

```bash
python app.py
```

Or with custom options:
```bash
uvicorn app:app --host 0.0.0.0 --port 5005 --reload
```

Access:
- üåê **Web Interface**: http://localhost:5005/
- üìö **API Docs (Swagger)**: http://localhost:5005/docs
- üìñ **ReDoc**: http://localhost:5005/redoc

---

## üîå API Reference

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/v1/audio/speech` | OpenAI-compatible speech synthesis |
| `POST` | `/v1/speech/audio` | Alternative speech endpoint |
| `POST` | `/speak` | Legacy endpoint (returns JSON) |
| `GET` | `/v1/voices` | List available voices |
| `GET` | `/health` | Health check endpoint |

### Speech Synthesis

**OpenAI-Compatible Endpoint:**
```bash
curl http://localhost:5005/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "orpheus-2026",
    "input": "Hello world! This is a test of the Orpheus TTS system.",
    "voice": "tara",
    "response_format": "wav",
    "speed": 1.0
  }' \
  --output speech.wav
```

**Alternative Endpoint:**
```bash
curl http://localhost:5005/v1/speech/audio \
  -H "Content-Type: application/json" \
  -d '{
    "model": "orpheus-2026",
    "input": "This endpoint also works perfectly!",
    "voice": "leo"
  }' \
  --output speech.wav
```

### Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `input` | string | *required* | Text to convert (max 8192 chars) |
| `model` | string | `"orpheus-2026"` | Model identifier |
| `voice` | string | `"tara"` | Voice selection |
| `response_format` | string | `"wav"` | Output format (only 'wav' supported) |
| `speed` | float | `1.0` | Speed factor (0.5-2.0) |

---

## üé≠ Available Voices

| Voice | Gender | Characteristics |
|-------|--------|-----------------|
| `tara` ‚≠ê | Female | Conversational, clear |
| `leah` | Female | Warm, gentle |
| `jess` | Female | Energetic, youthful |
| `leo` | Male | Authoritative, deep |
| `dan` | Male | Friendly, casual |
| `mia` | Female | Professional, articulate |
| `zac` | Male | Enthusiastic, dynamic |
| `zoe` | Female | Calm, soothing |

---

## üí´ Emotion Tags

Add expressive emotions to your speech:

```text
"Well, that's interesting <laugh> I hadn't thought of that before."
"Oh no <sigh> that's unfortunate news."
"What?! <gasp> I can't believe it!"
```

| Tag | Effect |
|-----|--------|
| `<laugh>` | Laughter |
| `<chuckle>` | Light chuckle |
| `<sigh>` | Sigh |
| `<gasp>` | Gasp |
| `<yawn>` | Yawn |
| `<groan>` | Groan |
| `<cough>` | Cough |
| `<sniffle>` | Sniffle |

---

## üîß Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ORPHEUS_API_URL` | `http://127.0.0.1:1234/v1/completions` | LLM inference endpoint |
| `ORPHEUS_API_TIMEOUT` | `120` | Request timeout (seconds) |

---

## üñ•Ô∏è LM Studio Setup

1. Download and install [LM Studio](https://lmstudio.ai/)
2. Download the **Orpheus-3b-0.1-ft-Q4_K_M-GGUF** model in the Discover tab
3. Load the model in the Developer Tab (starts API server automatically)

![LM Studio Orpheus Model](https://github.com/TheLocalLab/Orpheus-FastAPI-LMStudio/blob/8c9eb86b4c42a0ce60d8089c1b6a07d3635ae908/static/Lmstudio1.png)
![LM Studio Server API](https://github.com/TheLocalLab/Orpheus-FastAPI-LMStudio/blob/8c9eb86b4c42a0ce60d8089c1b6a07d3635ae908/static/Lmstudio.png)

---

## ‚ö° Performance & Optimization

### CPU and GPU Compatibility

Orpheus-FastAPI automatically detects your hardware and optimizes performance:

**GPU Support:**
- ‚úÖ **NVIDIA CUDA** - Optimal performance with CUDA-enabled GPUs (RTX series recommended)
- ‚úÖ **Apple Silicon (MPS)** - Native support for M1/M2/M3 chips
- ‚úÖ **CPU Fallback** - Runs efficiently on CPU when GPU is unavailable

**Automatic Optimization:**
- Detects GPU tier (High-end, Mid-range, Low-end) and adjusts batch sizes
- Configures memory buffers based on available VRAM
- Optimizes thread pools for CPU-only systems
- Adjusts token generation limits based on hardware capabilities

**Performance Tips:**
- **GPU Systems**: Install PyTorch with CUDA for best performance
- **Apple Silicon**: PyTorch automatically uses MPS backend
- **CPU Systems**: Enable multithreading, expects slower generation
- **High-end GPUs (RTX 4090, A100)**: Get 2-4x realtime generation speed
- **Mid-range GPUs (RTX 3070-4080)**: Get 1-2x realtime generation speed
- **CPU**: Expect slower than realtime, but still functional

### Memory Requirements

| Device Type | Min VRAM (GPU) | Min System RAM | Recommended | Max Audio Length |
|-------------|----------------|----------------|-------------|------------------|
| High-end GPU (24GB+) | 4GB* | 8GB | 16GB+ System RAM | 2+ minutes |
| Mid-range GPU (8-16GB) | 4GB* | 8GB | 12GB+ System RAM | 1.5 minutes |
| Low-end GPU (4-8GB) | 4GB* | 8GB | 12GB+ System RAM | 1 minute |
| CPU | N/A | 8GB | 16GB+ System RAM | 1 minute |

*The SNAC model requires ~4GB VRAM regardless of GPU tier. Larger VRAM allows for more parallel processing and longer audio generation.

---

## üîó Integration with OpenWebUI

Integrate with [OpenWebUI](https://github.com/open-webui/open-webui) for voice-enabled chat:

1. Start Orpheus-FASTAPI server
2. In OpenWebUI: **Admin Panel ‚Üí Settings ‚Üí Audio**
3. Change TTS from Web API to **OpenAI**
4. Set API Base URL to `http://localhost:5005`
5. API Key: `not-needed`
6. TTS Voice: `tara` (or any available voice)
7. TTS Model: `tts-1`

---

## üõ†Ô∏è Technical Details

### Architecture

This server acts as a frontend connecting to an external LLM inference server:

1. Text prompts are sent to the inference server
2. Generated tokens are converted to audio using SNAC codec
3. Optimized for RTX GPUs with CUDA acceleration

### Performance Optimizations

- Vectorized tensor operations
- Parallel processing with CUDA streams
- Efficient memory management
- Token and audio caching
- Optimized batch sizes

---

## üìú License

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

---

## üôè Credits

Originally created by [Lex-au](https://github.com/Lex-au/Orpheus-FastAPI)
Enhanced for 2026 Edition by the community.
